{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Klingon_Train.txt'\n",
    "file = open(filename, encoding=\"utf8\")\n",
    "text = file.read()\n",
    "\n",
    "f = open(\"README.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "words = text.split()\n",
    "\n",
    "#Counts how many times a tag appears in the file.\n",
    "dictionary = {}\n",
    "dictionary['<s>'] = 1\n",
    "\n",
    "#Counts the number of times each \"word\" appears as a specific tag.\n",
    "word_counts = {}\n",
    "\n",
    "#Create a string of tags only.\n",
    "tagOnly = \"<s> \"\n",
    "\n",
    "#Create a string of words only.\n",
    "wordOnly = \"\"\n",
    "\n",
    "f.write(\"Word Probabilities To Tags \\n\")\n",
    "#Collect counts\n",
    "for word in words:\n",
    "    newWord = word.split('/')\n",
    "    tagOnly = tagOnly + newWord[1] + \" \"\n",
    "    wordOnly = wordOnly + newWord[0] + \" \"\n",
    "    if word not in word_counts:\n",
    "        word_counts[word] = 1\n",
    "    else:\n",
    "        word_counts[word] = word_counts[word] + 1\n",
    "        \n",
    "    if newWord[1] not in dictionary:\n",
    "        dictionary[newWord[1]] = 1\n",
    "    else:\n",
    "        dictionary[newWord[1]] = dictionary[newWord[1]] + 1\n",
    "        \n",
    "for word in word_counts:\n",
    "    f.write(\"\\n\")\n",
    "    newWord = word.split('/')\n",
    "    for tag in dictionary:\n",
    "        if (newWord[0] + \"/\" + tag) != word:\n",
    "                f.write(tag + \" : \" + newWord[0] + \" --> \" + str(0.1) + '\\n')\n",
    "        else:\n",
    "                f.write(tag + \" : \" + newWord[0] + \" --> \" + str(word_counts[word]/dictionary[newWord[1]]+0.1) + '\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "\n",
    "#Create counts of bigrams\n",
    "bigram_dict = {}\n",
    "\n",
    "f.write(\"\\n \\n Tag Probability Given Previous Tag \\n\")\n",
    "\n",
    "theTags = tagOnly.split()\n",
    "for i in range(len(theTags) - 1):\n",
    "    if theTags[i] not in bigram_dict:\n",
    "        bigram_dict[theTags[i]] = {}\n",
    "        bigram_dict[theTags[i]][theTags[i+1]] = 1\n",
    "    else: \n",
    "        if theTags[i+1] not in bigram_dict[theTags[i]]:\n",
    "            bigram_dict[theTags[i]][theTags[i+1]] = 1\n",
    "        else:\n",
    "            bigram_dict[theTags[i]][theTags[i+1]] = bigram_dict[theTags[i]][theTags[i+1]] + 1\n",
    "\n",
    "            \n",
    "for entry in bigram_dict:\n",
    "    f.write(entry.upper() + \" Transition Probabilities \\n\")\n",
    "    f.write(\"________________________________________\" + \"\\n\")\n",
    "    for tag in dictionary:\n",
    "        if tag not in bigram_dict[entry]:\n",
    "                f.write(entry + \" \" + tag + \": \" + str(0.1) + \"\\n\")\n",
    "    for subentry in bigram_dict[entry]:\n",
    "        f.write(entry + \" \" + subentry + \": \" + str((bigram_dict[entry][subentry]/dictionary[entry]) + 0.1) + \"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part 3\n",
    "\n",
    "sentence = wordOnly.split()\n",
    "\n",
    "msg = \"teraâ€™ngan legh yaS\".split()\n",
    "\n",
    "keys = ['N', 'V', 'PRO', 'CONJ']\n",
    "\n",
    "T = len(keys)\n",
    "\n",
    "W = len(msg)\n",
    "\n",
    "Score = np.zeros((T, W))\n",
    "\n",
    "Backptr = np.zeros((T, W))\n",
    "\n",
    "finalString = \"\"\n",
    "\n",
    "#Initialization Step\n",
    "for t in range(0, T):\n",
    "    finalString = msg[0] + '/' + keys[t]\n",
    "    if msg[0] not in wordOnly.split():\n",
    "        if keys[t] not in bigram_dict['<s>']:\n",
    "            Score[t][0] = ((0.1/(1))+0.1) * ((0/1 + 0.1))\n",
    "    else:\n",
    "        if finalString not in word_counts:\n",
    "            if keys[t] not in bigram_dict['<s>']:\n",
    "                Score[t][0] = ((0/(1))+0.1) * ((0/1) + 0.1)\n",
    "            else:\n",
    "                Score[t][0] = ((0/(1))+0.1) * ((bigram_dict['<s>'][keys[t]]/1) + 0.1)\n",
    "        else:\n",
    "            Score[t][0] = ((word_counts[finalString]/(1))+0.1) * (((bigram_dict['<s>'][keys[t]]/1)) + 0.1)\n",
    "    \n",
    "    Backptr[t][0] = 0\n",
    "    \n",
    "#Dynamic Step        \n",
    "for w in range(1, W):\n",
    "    for t in range(0, T):\n",
    "        finalString = msg[w] + '/' + keys[t]\n",
    "        maxScore = 0\n",
    "        for j in range(0, T):\n",
    "            if keys[j] in bigram_dict[keys[t]]:\n",
    "                if Score[j][w-1] * bigram_dict[keys[t]][keys[j]] > maxScore:\n",
    "                    maxScore = Score[j][w-1] * bigram_dict[keys[t]][keys[j]]\n",
    "                    Backptr[t][w] = j #i am doing index 0 so adding 1. In this case 1 = 0 in our array.\n",
    "            else:\n",
    "                if Score[j][w-1] * 0.1 > maxScore:\n",
    "                    maxScore = Score[j][w-1] * 0.1\n",
    "                    Backptr[t][w] = j  #i am doing index 0 so adding 1. In this case 1 = 0 in our array.\n",
    "        \n",
    "        #Since we know the last 2 words are just not in our training at all, We can simply address case where the word is not \n",
    "        #present inour training corpus and leave the implementation of other cases until we wish to expand on this.\n",
    "        if msg[w] not in wordOnly.split():\n",
    "            Score[t][w] = ((0.1/dictionary[keys[t]])+0.1) * maxScore\n",
    "\n",
    "#Backtracking step.           \n",
    "Sequence = [0, 0, 0]\n",
    "\n",
    "w = W - 2\n",
    "\n",
    "while w >= 0:\n",
    "    Sequence[w] = int(Backptr[Sequence[w+1]][w+1])\n",
    "    w = w - 1\n",
    "\n",
    "f.write(\"\\n\")\n",
    "f.write(\"The likely tags for the sentence seem to be \" + msg[0] + \"/\" + keys[int(Sequence[0])] + \" \" + msg[1] + \"/\" + keys[int(Sequence[1])] + \" \" + msg[2] + \"/\" + keys[int(Sequence[2])])\n",
    "f.write(\"\\n The message definitely starts with 'Human', but since we do not know what legh and yaS mean, we can only speculate. The subject seems to be yaS.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
